{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config.n_future_tokens = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login e461a6a3bca9f7cec3390a40dc10cdf576ce3252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_llama_for_multitoken():\n",
    "    import copy\n",
    "    def patched_forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        hidden_states = outputs[0]  # ожидаем форму (batch_size, seq_len, hidden_size)\n",
    "        n_future_tokens = getattr(self.config, \"n_future_tokens\", 1)\n",
    "\n",
    "        if n_future_tokens > 1:\n",
    "            trunk_states = hidden_states\n",
    "            latents = [trunk_states]\n",
    "\n",
    "            if not hasattr(self, \"extra_heads\"):\n",
    "                last_layer = self.model.layers[-1]\n",
    "                self.extra_heads = torch.nn.ModuleList([\n",
    "                    copy.deepcopy(last_layer) for _ in range(n_future_tokens - 1)\n",
    "                ])\n",
    "\n",
    "            if \"position_ids\" not in kwargs or kwargs[\"position_ids\"] is None:\n",
    "                batch_size, seq_len, _ = trunk_states.shape\n",
    "                kwargs[\"position_ids\"] = torch.arange(seq_len, device=trunk_states.device)\\\n",
    "                    .unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "            for head in self.extra_heads:\n",
    "                if isinstance(trunk_states, tuple):\n",
    "                    trunk_states = trunk_states[0]\n",
    "                batch_size, seq_len, _ = trunk_states.shape\n",
    "                local_kwargs = kwargs.copy()\n",
    "                local_kwargs['position_ids'] = torch.arange(seq_len, device=trunk_states.device)\\\n",
    "                    .unsqueeze(0).expand(batch_size, seq_len)\n",
    "                if attention_mask is not None:\n",
    "                    if attention_mask.ndim == 2:\n",
    "                        local_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "                    else:\n",
    "                        local_attention_mask = attention_mask\n",
    "                    local_attention_mask = local_attention_mask.float()\n",
    "                else:\n",
    "                    local_attention_mask = None\n",
    "                output = head(trunk_states, attention_mask=local_attention_mask, **local_kwargs)\n",
    "                if isinstance(output, tuple):\n",
    "                    trunk_states = output[0]\n",
    "                else:\n",
    "                    trunk_states = output\n",
    "                latents.append(trunk_states)\n",
    "\n",
    "            hidden_states = torch.stack(latents, dim=2)  # (batch_size, seq_len, n_future_tokens, hidden_size)\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        return logits\n",
    "\n",
    "    LlamaForCausalLM.forward = patched_forward\n",
    "    print(\"LlamaForCausalLM patched for multi-token generation.\")\n",
    "\n",
    "# Применяем патч (для новых экземпляров модели)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальная модель (generate):\n",
      "Время генерации: 13.7326 сек\n",
      "Сгенерированный текст: Сегодня на улице, в стенах и в ушах слышится музыка. В каждом из этих\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загружаем токенайзер и оригинальную модель LLaMA\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model_original = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Промпт для генерации\n",
    "prompt = \"Сегодня на улице\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model_original.device) for k, v in inputs.items()}\n",
    "\n",
    "# Генерация с помощью стандартного метода generate\n",
    "max_new_tokens = 20\n",
    "start_time = time.time()\n",
    "generated_ids_orig = model_original.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "orig_time = time.time() - start_time\n",
    "\n",
    "text_orig = tokenizer.decode(generated_ids_orig[0], skip_special_tokens=True)\n",
    "print(\"Оригинальная модель (generate):\")\n",
    "print(\"Время генерации: {:.4f} сек\".format(orig_time))\n",
    "print(\"Сгенерированный текст:\", text_orig)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Патченная модель с multi-token генерацией\n",
    "# -------------------------------\n",
    "\n",
    "# Здесь предполагается, что ранее была выполнена функция patch_llama_for_multitoken(),\n",
    "# которая заменяет метод forward в классе LlamaForCausalLM.\n",
    "# (Если патч ещё не применён, его нужно выполнить до создания модели.)\n",
    "\n",
    "# Загружаем модель – она уже будет использовать патч, т.к. класс был модифицирован.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM patched for multi-token generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "def patch_llama_for_multitoken():\n",
    "    import copy\n",
    "    def patched_forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        hidden_states = outputs[0]  # ожидаем форму (batch_size, seq_len, hidden_size)\n",
    "        n_future_tokens = getattr(self.config, \"n_future_tokens\", 1)\n",
    "\n",
    "        if n_future_tokens > 1:\n",
    "            trunk_states = hidden_states\n",
    "            latents = [trunk_states]\n",
    "\n",
    "            if not hasattr(self, \"extra_heads\"):\n",
    "                last_layer = self.model.layers[-1]\n",
    "                self.extra_heads = torch.nn.ModuleList([\n",
    "                    copy.deepcopy(last_layer) for _ in range(n_future_tokens - 1)\n",
    "                ])\n",
    "\n",
    "            if \"position_ids\" not in kwargs or kwargs[\"position_ids\"] is None:\n",
    "                batch_size, seq_len, _ = trunk_states.shape\n",
    "                kwargs[\"position_ids\"] = torch.arange(seq_len, device=trunk_states.device)\\\n",
    "                    .unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "            for head in self.extra_heads:\n",
    "                if isinstance(trunk_states, tuple):\n",
    "                    trunk_states = trunk_states[0]\n",
    "                batch_size, seq_len, _ = trunk_states.shape\n",
    "                local_kwargs = kwargs.copy()\n",
    "                local_kwargs['position_ids'] = torch.arange(seq_len, device=trunk_states.device)\\\n",
    "                    .unsqueeze(0).expand(batch_size, seq_len)\n",
    "                if attention_mask is not None:\n",
    "                    if attention_mask.ndim == 2:\n",
    "                        local_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "                    else:\n",
    "                        local_attention_mask = attention_mask\n",
    "                    local_attention_mask = local_attention_mask.float()\n",
    "                else:\n",
    "                    local_attention_mask = None\n",
    "                output = head(trunk_states, attention_mask=local_attention_mask, **local_kwargs)\n",
    "                if isinstance(output, tuple):\n",
    "                    trunk_states = output[0]\n",
    "                else:\n",
    "                    trunk_states = output\n",
    "                latents.append(trunk_states)\n",
    "\n",
    "            hidden_states = torch.stack(latents, dim=2)  # (batch_size, seq_len, n_future_tokens, hidden_size)\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        return logits\n",
    "\n",
    "    LlamaForCausalLM.forward = patched_forward\n",
    "    print(\"LlamaForCausalLM patched for multi-token generation.\")\n",
    "\n",
    "# Применяем патч\n",
    "patch_llama_for_multitoken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_patched = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# Устанавливаем параметр для multi-token предсказания (например, 10 токенов за шаг)\n",
    "model_patched.config.n_future_tokens = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Патченная модель (n_future_tokens = 10):\n",
      "Время генерации: 2.2791 сек\n",
      "Сгенерированный текст: Сегодня на улице в появ появ появ появ появ появ����娇�������\n"
     ]
    }
   ],
   "source": [
    "def generate_multitoken(model, input_ids, max_new_tokens, n_future_tokens):\n",
    "    \"\"\"\n",
    "    Генерирует текст, используя модель с патченной forward, которая возвращает логиты\n",
    "    формы (batch_size, seq_len, n_future_tokens, vocab_size).\n",
    "    На каждом шаге выбираются предсказания для всех future-токенов, которые затем дописываются к последовательности.\n",
    "    \"\"\"\n",
    "    generated = input_ids  # Здесь input_ids уже является тензором!\n",
    "    steps = max_new_tokens // n_future_tokens\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            outputs = model(generated)\n",
    "            # last_logits имеет форму: (batch_size, n_future_tokens, vocab_size)\n",
    "            last_logits = outputs[0, -1, :, :]\n",
    "            # Выбираем argmax для каждого из n_future_tokens\n",
    "            predicted_tokens = torch.argmax(last_logits, dim=-1)  # (batch_size, n_future_tokens)\n",
    "            # Если батч равен 1, убеждаемся, что predicted_tokens имеет форму (1, n_future_tokens)\n",
    "            if predicted_tokens.dim() == 1:\n",
    "                predicted_tokens = predicted_tokens.unsqueeze(0)\n",
    "            generated = torch.cat([generated, predicted_tokens], dim=1)\n",
    "    return generated\n",
    "\n",
    "# Подготавливаем входные данные: извлекаем тензор 'input_ids'\n",
    "prompt = \"Сегодня на улице\"\n",
    "# Извлекаем тензор, а не весь BatchEncoding\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model_patched.device)\n",
    "\n",
    "max_new_tokens = 20\n",
    "start_time = time.time()\n",
    "generated_ids_patched = generate_multitoken(model_patched, input_ids, \n",
    "                                            max_new_tokens=max_new_tokens, \n",
    "                                            n_future_tokens=model_patched.config.n_future_tokens)\n",
    "patched_time = time.time() - start_time\n",
    "\n",
    "text_patched = tokenizer.decode(generated_ids_patched[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Патченная модель (n_future_tokens = {}):\".format(model_patched.config.n_future_tokens))\n",
    "print(\"Время генерации: {:.4f} сек\".format(patched_time))\n",
    "print(\"Сгенерированный текст:\", text_patched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве примера используем датасет wikitext-2 (raw версия)\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "# 4. Настройка параметров обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "# 3. Загрузка модели и токенайзера\n",
    "#############################################\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Для обучения устанавливаем n_future_tokens = 10,\n",
    "# но для вычисления loss используем только первую prediction (индекс 0)\n",
    "model.config.n_future_tokens = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"llama_mtp\", config={\n",
    "    \"model_name\": \"meta-llama/Llama-3.2-1B\",\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 2,\n",
    "    \"n_future_tokens\": 10,\n",
    "    \"block_size\": 128,\n",
    "})\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 4. Подготовка датасета\n",
    "#############################################\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Группировка токенов в блоки фиксированной длины (block_size)\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = LMDataset(lm_dataset)\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:81hyw2mx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-microwave-1</strong> at: <a href='https://wandb.ai/alexwortega/llama_mtp/runs/81hyw2mx' target=\"_blank\">https://wandb.ai/alexwortega/llama_mtp/runs/81hyw2mx</a><br/> View project at: <a href='https://wandb.ai/alexwortega/llama_mtp' target=\"_blank\">https://wandb.ai/alexwortega/llama_mtp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250207_170906-81hyw2mx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:81hyw2mx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexwortega/llama_mtp/runs/oa52gjqf' target=\"_blank\">desert-forest-2</a></strong> to <a href='https://wandb.ai/alexwortega/llama_mtp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexwortega/llama_mtp' target=\"_blank\">https://wandb.ai/alexwortega/llama_mtp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexwortega/llama_mtp/runs/oa52gjqf' target=\"_blank\">https://wandb.ai/alexwortega/llama_mtp/runs/oa52gjqf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 36718/36718 [00:01<00:00, 19829.06 examples/s]\n",
      "Map: 100%|██████████| 36718/36718 [00:08<00:00, 4375.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 300, Loss: 0.5558\n",
      "Step 300 prediction via generate_multitoken: Сегодня на улице matches          matches         \n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-300.bin\n",
      "Epoch 1, Step 600, Loss: 0.1309\n",
      "Step 600 prediction via generate_multitoken: Сегодня на улицеgesgesgesssssssssssssadalafiladalafiladalafiladalafiladalafil\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-600.bin\n",
      "Epoch 1, Step 900, Loss: 0.1060\n",
      "Step 900 prediction via generate_multitoken: Сегодня на улицеéééééеentifierentifier]';\n",
      "]';\n",
      "JRJRJRJR |/ |/ |/>>)hursthurst\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-900.bin\n",
      "Epoch 1, Step 1200, Loss: 0.0993\n",
      "Step 1200 prediction via generate_multitoken: Сегодня на улицеecec de������ Evet Evet Evet Evet Evet Evet Evet Evet Evet Evet Are\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-1200.bin\n",
      "Epoch 1, Step 1500, Loss: 0.0861\n",
      "Step 1500 prediction via generate_multitoken: Сегодня на улицеageageeeeehehehehehehehehehehehehehehe\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-1500.bin\n",
      "Epoch 1, Step 1800, Loss: 0.1962\n",
      "Step 1800 prediction via generate_multitoken: Сегодня на улицеatesatesatesatesatesatesatesatesatesatesatesatesatesatesatesatesatesodesodes Catholics\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-1800.bin\n",
      "Epoch 1, Step 2100, Loss: 0.1164\n",
      "Step 2100 prediction via generate_multitoken: Сегодня на улицеededededhehehehehehehehehehehehehehehehe\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-2100.bin\n",
      "Epoch 1, Step 2400, Loss: 0.1348\n",
      "Step 2400 prediction via generate_multitoken: Сегодня на улице������icesicesicesicesicesicesicesicesicesicesicesicesicesices\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-2400.bin\n",
      "Epoch 1, Step 2700, Loss: 0.0662\n",
      "Step 2700 prediction via generate_multitoken: Сегодня на улицеceelelelelisisisasuuuuuuuuuadalafiladalafil\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-2700.bin\n",
      "Epoch 1, Step 3000, Loss: 0.1198\n",
      "Step 3000 prediction via generate_multitoken: Сегодня на улицеiediediediediedetetelelelelelelelelelelelelel\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-3000.bin\n",
      "Epoch 1, Step 3300, Loss: 0.1316\n",
      "Step 3300 prediction via generate_multitoken: Сегодня на улицеcéåاءייייייייייייייייי\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-3300.bin\n",
      "Epoch 1, Step 3600, Loss: 0.0472\n",
      "Step 3600 prediction via generate_multitoken: Сегодня на улицеercercerc each each each each each each each each each each each each each each each each each\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-3600.bin\n",
      "Epoch 1, Step 3900, Loss: 0.1235\n",
      "Step 3900 prediction via generate_multitoken: Сегодня на улицеicesicesicesicesicesicesicesicesicesicesicesicesicesicesicesicesicesicesicesices\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-3900.bin\n",
      "Epoch 1, Step 4200, Loss: 0.2505\n",
      "Step 4200 prediction via generate_multitoken: Сегодня на улицеogofofofofofofofof of of of of of of of of of of of\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-4200.bin\n",
      "Epoch 1, Step 4500, Loss: 0.0246\n",
      "Step 4500 prediction via generate_multitoken: Сегодня на улицеbackthinkthinkthinkthinkthinkthinkagainagainagain again again again again again again again again again again\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-4500.bin\n",
      "Epoch 1, Step 4800, Loss: 0.1313\n",
      "Step 4800 prediction via generate_multitoken: Сегодня на улицеiringohlóngičierList| MartTags专 Expert „ Special专专专专专专专\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-4800.bin\n",
      "Epoch 1, Step 5100, Loss: 0.5184\n",
      "Step 5100 prediction via generate_multitoken: Сегодня на улице Carey Carey Careyäääääääääääääääää\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-5100.bin\n",
      "Epoch 1, Step 5400, Loss: 0.1123\n",
      "Step 5400 prediction via generate_multitoken: Сегодня на улицеungungatedatedatedricricricricricricricricricricricricricälläll\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-5400.bin\n",
      "Epoch 1, Step 5700, Loss: 0.1107\n",
      "Step 5700 prediction via generate_multitoken: Сегодня на улице worship worship worship worship worship worship worship worship worship worship worship worship worship worship worship worship worship worship worship worship\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-5700.bin\n",
      "Epoch 1, Step 6000, Loss: 0.0631\n",
      "Step 6000 prediction via generate_multitoken: Сегодня на улицеongahiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphip\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-6000.bin\n",
      "Epoch 1, Step 6300, Loss: 0.1181\n",
      "Step 6300 prediction via generate_multitoken: Сегодня на улице both both both both both both both both both both both both both both both both both both both both\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-6300.bin\n",
      "Epoch 1, Step 6600, Loss: 0.0581\n",
      "Step 6600 prediction via generate_multitoken: Сегодня на улицеangangangangngngngitre thee thee thee thee thee thee thee thee thee thee thee thee\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-6600.bin\n",
      "Epoch 1, Step 6900, Loss: 0.1512\n",
      "Step 6900 prediction via generate_multitoken: Сегодня на улице����広広広広広����広広広広広\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-6900.bin\n",
      "Epoch 1, Step 7200, Loss: 0.1388\n",
      "Step 7200 prediction via generate_multitoken: Сегодня на улице = = =広広広広letalletalletal Credits Republicanletalletalletal Credits Credits Eventually Ab It\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-7200.bin\n",
      "Epoch 1, Step 7500, Loss: 0.0672\n",
      "Step 7500 prediction via generate_multitoken: Сегодня на улицеs of of of of of of of of of of of of of of of of of of of\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-7500.bin\n",
      "Epoch 1, Step 7800, Loss: 0.0994\n",
      "Step 7800 prediction via generate_multitoken: Сегодня на улице Mary Mary Mary Mary by by by by by being being being being being being being being being being being\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-7800.bin\n",
      "Epoch 1, Step 8100, Loss: 0.1045\n",
      "Step 8100 prediction via generate_multitoken: Сегодня на улице Gal Gal Gal Gal Gal Michael K of I desn desnVAS desn desn y He desn desn desn desn\n",
      "Checkpoint saved at ./llama_finetuned_pure_torch/checkpoint-8100.bin\n",
      "Epoch 1, Step 8400, Loss: 0.1004\n",
      "Step 8400 prediction via generate_multitoken: Сегодня на улице envy envy envy envy envy envy envylrelligent Model Model Model Model Model Model Model Model Model Model Model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:626] . unexpected pos 2812446272 vs 2812446160",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/torch/serialization.py:944\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 944\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/torch/serialization.py:1216\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/16: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;66;03m# Сохранение чекпоинта\u001b[39;00m\n\u001b[1;32m    134\u001b[0m         checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 135\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[0;32m~/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/torch/serialization.py:943\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    940\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 943\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "File \u001b[0;32m~/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/torch/serialization.py:784\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 2812446272 vs 2812446160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 99, in _run\n",
      "    self._process(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 278, in _process\n",
      "    self._hm.handle(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 150, in handle\n",
      "    handler(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 158, in handle_request\n",
      "    logger.debug(f\"handle_request: {request_type}\")\n",
      "Message: 'handle_request: status_report'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 882, in finish\n",
      "    logger.info(\"shutting down handler\")\n",
      "Message: 'shutting down handler'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 884, in finish\n",
      "    self._system_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 203, in finish\n",
      "    logger.info(\"Stopping system monitor\")\n",
      "Message: 'Stopping system monitor'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 179, in _start\n",
      "    logger.debug(\"Finished system metrics aggregation loop\")\n",
      "Message: 'Finished system metrics aggregation loop'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 884, in finish\n",
      "    self._system_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n",
      "    asset.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 163, in finish\n",
      "    self.metrics_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 200, in finish\n",
      "    logger.info(f\"Joined {thread_name} monitor\")\n",
      "Message: 'Joined cpu monitor'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 183, in _start\n",
      "    logger.debug(\"Publishing last batch of metrics\")\n",
      "Message: 'Publishing last batch of metrics'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 884, in finish\n",
      "    self._system_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n",
      "    asset.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/disk.py\", line 210, in finish\n",
      "    self.metrics_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 200, in finish\n",
      "    logger.info(f\"Joined {thread_name} monitor\")\n",
      "Message: 'Joined disk monitor'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 884, in finish\n",
      "    self._system_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n",
      "    asset.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/gpu.py\", line 388, in finish\n",
      "    self.metrics_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 200, in finish\n",
      "    logger.info(f\"Joined {thread_name} monitor\")\n",
      "Message: 'Joined gpu monitor'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 884, in finish\n",
      "    self._system_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n",
      "    asset.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/memory.py\", line 152, in finish\n",
      "    self.metrics_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 200, in finish\n",
      "    logger.info(f\"Joined {thread_name} monitor\")\n",
      "Message: 'Joined memory monitor'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 281, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/handler.py\", line 884, in finish\n",
      "    self._system_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n",
      "    asset.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/network.py\", line 96, in finish\n",
      "    self.metrics_monitor.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 200, in finish\n",
      "    logger.info(f\"Joined {thread_name} monitor\")\n",
      "Message: 'Joined network monitor'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1631, in finish\n",
      "    logger.info(\"shutting down sender\")\n",
      "Message: 'shutting down sender'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n",
      "    self.dispatch_events(self.event_queue, self.timeout)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n",
      "    handler.dispatch(event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n",
      "    _method_map[event_type](event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n",
      "    logger.info(f\"file/dir modified: { event.src_path}\")\n",
      "Message: 'file/dir modified: /home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/output.log'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 358, in finish\n",
      "    logger.info(\"shutting down directory watcher\")\n",
      "Message: 'shutting down directory watcher'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n",
      "    self.dispatch_events(self.event_queue, self.timeout)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n",
      "    handler.dispatch(event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n",
      "    _method_map[event_type](event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n",
      "    logger.info(f\"file/dir modified: { event.src_path}\")\n",
      "Message: 'file/dir modified: /home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/output.log'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 388, in finish\n",
      "    logger.info(\"scan: %s\", self._dir)\n",
      "Message: 'scan: %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n",
      "    logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "Message: 'scan save: %s %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/wandb-metadata.json', 'wandb-metadata.json')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n",
      "    logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "Message: 'scan save: %s %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/requirements.txt', 'requirements.txt')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n",
      "    logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "Message: 'scan save: %s %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/output.log', 'output.log')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n",
      "    logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "Message: 'scan save: %s %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/config.yaml', 'config.yaml')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1636, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n",
      "    logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "Message: 'scan save: %s %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/wandb-summary.json', 'wandb-summary.json')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1639, in finish\n",
      "    self._pusher.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/file_pusher.py\", line 169, in finish\n",
      "    logger.info(\"shutting down file pusher\")\n",
      "Message: 'shutting down file pusher'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1640, in finish\n",
      "    self._pusher.join()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/file_pusher.py\", line 175, in join\n",
      "    logger.info(\"waiting for file pusher\")\n",
      "Message: 'waiting for file pusher'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 92, in _worker\n",
      "    work_item.run()\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 223, in run_and_notify\n",
      "    self._do_upload(event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 243, in _do_upload\n",
      "    job.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 56, in run\n",
      "    self.push()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 130, in push\n",
      "    logger.info(\"Uploaded file %s\", self.save_path)\n",
      "Message: 'Uploaded file %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/requirements.txt',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 92, in _worker\n",
      "    work_item.run()\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 223, in run_and_notify\n",
      "    self._do_upload(event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 243, in _do_upload\n",
      "    job.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 56, in run\n",
      "    self.push()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 130, in push\n",
      "    logger.info(\"Uploaded file %s\", self.save_path)\n",
      "Message: 'Uploaded file %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/output.log',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 92, in _worker\n",
      "    work_item.run()\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 223, in run_and_notify\n",
      "    self._do_upload(event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 243, in _do_upload\n",
      "    job.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 56, in run\n",
      "    self.push()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 130, in push\n",
      "    logger.info(\"Uploaded file %s\", self.save_path)\n",
      "Message: 'Uploaded file %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/wandb-summary.json',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 92, in _worker\n",
      "    work_item.run()\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 223, in run_and_notify\n",
      "    self._do_upload(event)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/step_upload.py\", line 243, in _do_upload\n",
      "    job.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 56, in run\n",
      "    self.push()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/filesync/upload_job.py\", line 130, in push\n",
      "    logger.info(\"Uploaded file %s\", self.save_path)\n",
      "Message: 'Uploaded file %s'\n",
      "Arguments: ('/home/alexw/alexw/multi_token/wandb/run-20250207_170938-oa52gjqf/files/config.yaml',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1643, in finish\n",
      "    self._fs.finish(self._exit_code)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/file_stream.py\", line 601, in finish\n",
      "    logger.info(\"file stream finish called\")\n",
      "Message: 'file stream finish called'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 330, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/sender.py\", line 1643, in finish\n",
      "    self._fs.finish(self._exit_code)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/file_stream.py\", line 605, in finish\n",
      "    logger.info(\"file stream finish is done\")\n",
      "Message: 'file stream finish is done'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n",
      "    self._target(**self._kwargs)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 173, in wandb_internal\n",
      "    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\n",
      "Message: 'Thread WriterThread:'\n",
      "Arguments: ()\n",
      "Thread WriterThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py\", line 99, in _run\n",
      "    self._process(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/internal.py\", line 379, in _process\n",
      "    self._wm.write(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/writer.py\", line 154, in write\n",
      "    write_handler(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/writer.py\", line 135, in _write\n",
      "    self._write_record(record)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/writer.py\", line 109, in _write_record\n",
      "    ret = self._ds.write(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/datastore.py\", line 291, in write\n",
      "    ret = self._write_data(s)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/datastore.py\", line 247, in _write_data\n",
      "    self._write_record(s)\n",
      "  File \"/home/alexw/InternShips/qklent/medusa/speculative-decoding-medusa-example/medusa_venv/lib/python3.12/site-packages/wandb/sdk/internal/datastore.py\", line 226, in _write_record\n",
      "    self._fp.write(s)\n",
      "OSError: [Errno 28] No space left on device\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "# 5. Определение функции generate_multitoken\n",
    "#############################################\n",
    "def generate_multitoken(model, inputs, max_new_tokens, n_future_tokens):\n",
    "    \"\"\"\n",
    "    Генерирует текст с использованием патченной модели.\n",
    "    Вход inputs – тензор input_ids.\n",
    "    На каждом шаге вызывается модель, которая возвращает логиты формы\n",
    "      (batch_size, seq_len, n_future_tokens, vocab_size),\n",
    "    и выбирается argmax по последней позиции для всех future-токенов.\n",
    "    \"\"\"\n",
    "    generated = inputs  # inputs уже является тензором\n",
    "    steps = max_new_tokens // n_future_tokens\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            outputs = model(generated)\n",
    "            # Логиты для последней позиции: (batch_size, n_future_tokens, vocab_size)\n",
    "            last_logits = outputs[0, -1, :, :]\n",
    "            predicted_tokens = torch.argmax(last_logits, dim=-1)  # (batch_size, n_future_tokens)\n",
    "            if predicted_tokens.dim() == 1:\n",
    "                predicted_tokens = predicted_tokens.unsqueeze(0)\n",
    "            generated = torch.cat([generated, predicted_tokens], dim=1)\n",
    "    return generated\n",
    "\n",
    "#############################################\n",
    "# 6. Настройка оптимизатора и обучение с предиктом и чекпоинтингом\n",
    "#############################################\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "global_step = 0\n",
    "print_every = 300  # каждые 300 шагов выводим информацию, предсказываем через generate_multitoken и сохраняем чекпоинт\n",
    "output_dir = \"./llama_finetuned_pure_torch\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=None, position_ids=None)\n",
    "        # Если n_future_tokens > 1, выход logits имеет форму:\n",
    "        # (batch_size, seq_len, n_future_tokens, vocab_size)\n",
    "        # Для loss используем только первую prediction (индекс 0)\n",
    "        if outputs.dim() == 4:\n",
    "            logits = outputs[:, :, 0, :]\n",
    "        else:\n",
    "            logits = outputs\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1),\n",
    "                             )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {global_step}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Генерация sample-текста через generate_multitoken\n",
    "            sample_prompt = \"A story about a cat:\"\n",
    "            sample_input_ids = tokenizer(sample_prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "            generated_ids = generate_multitoken(model, sample_input_ids,\n",
    "                                                max_new_tokens=20,\n",
    "                                                n_future_tokens=model.config.n_future_tokens)\n",
    "            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            print(f\"Step {global_step} prediction via generate_multitoken: {generated_text}\")\n",
    "            \n",
    "            # Логирование в wandb\n",
    "            wandb.log({\n",
    "                \"global_step\": global_step,\n",
    "                \"loss\": loss.item(),\n",
    "                \"sample_prediction\": generated_text,\n",
    "            })\n",
    "            \n",
    "            # Сохранение чекпоинта\n",
    "            checkpoint_path = f\"{output_dir}/checkpoint-{global_step}.bin\"\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "            \n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "    wandb.log({\"epoch\": epoch+1, \"average_loss\": avg_loss})\n",
    "\n",
    "#############################################\n",
    "# 7. Сохранение финальной модели\n",
    "#############################################\n",
    "final_model_path = output_dir + \"/pytorch_model_final.bin\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Final model saved in:\", output_dir)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medusa-venv",
   "language": "python",
   "name": "medusa-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
